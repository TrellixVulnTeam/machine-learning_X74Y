{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수치 안정성(numerical stability) 및 초기화\n",
    "\n",
    "지금까지 우리는 다층 퍼셉트론(multilayer perception)을 구현하는데 필요한 도구, 회귀와 분류의 문제를 어떻게 풀 수 있는지, 그리고 모델의 용량을 어떻게 제어해야하는지에 대해서 다뤘습니다. 하지만, 파라미터의 초기화는 당연한 것으로 간주하면서, 특별히 중요하지 않은 것으로 단순하게 가정했습니다. 이 절에서는 이것들에 대해서 자세히 살펴보고, 유용한 경험적 방법론에 대해서 논의하겠습니다.\n",
    "\n",
    "두번째로 우리는 활성화 함수(activation function) 선택에 큰 관심을 두지 않았습니다. 실제로 얕은 네트워크에서는 크게 중요하지 않지만, 딥 네트워크 (deep network)에서는 비선형성과 초기화의 선택이 최적화 알고리즘을 빠르게 수렴시키는데 중요한 역할을 합니다. 이 이슈들을 중요하게 생각하지 않으면 그래디언트 소실(vanishing) 또는 폭발(exploding)이 발생할 수 있습니다.\n",
    "\n",
    "## 그래디언트 소실(vanishing)과 폭발(exploding)\n",
    "\n",
    "입력이  $\\mathbf{x}$ , 출력이  $\\mathbf{o}$ 이고 $d$ 층을 갖는 딥 네트워크를 예로 들겠습니다. 각 층은 다음을 만족합니다.\n",
    "\n",
    "$$\\mathbf{h}^{t+1} = f_t (\\mathbf{h}^t) \\text{ 이고, 따라서 } \\mathbf{o} = f_d \\circ \\ldots \\circ f_1(\\mathbf{x})$$\n",
    "\n",
    "모든 활성화(activation)들과 입력들이 벡터인 경우, $t$ 번째 층의 함수 $f_t$ 와 관련된 파라미터 $\\mathbf{W}_t$ 의 임의의 세트에 대한  $\\mathbf{o}$ 의 그래디언트(gradient)는 다음과 같이 표현됩니다.\n",
    "\n",
    "$$\\partial_{\\mathbf{W}_t} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{d-1}} \\mathbf{h}^d}_{:= \\mathbf{M}_d} \\cdot \\ldots \\cdot \\underbrace{\\partial_{\\mathbf{h}^{t}} \\mathbf{h}^{t+1}}_{:= \\mathbf{M}_t} \\underbrace{\\partial_{\\mathbf{W}_t} \\mathbf{h}^t}_{:= \\mathbf{v}_t}.$$\n",
    "\n",
    "다르게 말하면, 위 공식은 $d-t$ 개의 행렬  $\\mathbf{M}_d \\cdot \\ldots \\cdot \\mathbf{M}_t$ 과 그래디언트(gradient) 벡터  $\\mathbf{v}_t$ 의 곱입니다. 너무 많은 확률을 곱할 때 산술적인 언더플로우(underflow)를 경험할 때와 비슷한 상황이 발생합니다. 이 문제를 로그 공간으로 전환시켜서, 즉 문제를 가수(mantissa)에서 수치 표현의 지수로 이동시켜서 완화할 수 있었습니다. 처음에 행렬들 $M_t$ 은 다양한 고유값(eigenvalue)들을 갖을 것입니다. 어떤 것들은 작을 수도, 어떤 것은 클 수도 있습니다. 특히 그것들의 곱이 아주 크거나 아주 작을 수도 있습니다. 이것은 수치적인 표현의 문제일 뿐만 아니라 최적화 알고리즘이 수렴되지 않을 수 있다는 것을 의미합니다. 아주 큰 그래디언트(gradient)가 되거나, 너무 조금씩 업데이트가 되기도 합니다. 앞의 경우에는, 파라미터가 너무 커질 것이고, 후자의 경우에는 그래디언트 소실(vanishing gradient)이 되어버려서 더 이상 의미 있는 진척을 만들어 내지 못하게 됩니다.\n",
    "\n",
    "### 그래디언트 폭발(exploding gradient)\n",
    "\n",
    "좀 더 자세히 설명해 보겠습니다. 하나의 행렬을 선택한 후 100개의 가우시안 랜덤 행렬을 선택해서 모두 곱합니다. 우리가 선택한 스캐일링으로 인해서 행렬의 곱은 너무 커지게 됩니다. 이러한 일이 딥 네트워크에서 발생한다면, 알고리즘을 수렴하게 만들기 어려워집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single matrix \n",
      "[[ 2.2122064   0.7740038   1.0434405   1.1839255 ]\n",
      " [ 1.8917114  -1.2347414  -1.771029   -0.45138445]\n",
      " [ 0.57938355 -1.856082   -1.9768796  -0.20801921]\n",
      " [ 0.2444218  -0.03716067 -0.48774993 -0.02261727]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "After multiplying 100 matrices \n",
      "[[ 3.1575275e+20 -5.0052276e+19  2.0565092e+21 -2.3741922e+20]\n",
      " [-4.6332600e+20  7.3445046e+19 -3.0176513e+21  3.4838066e+20]\n",
      " [-5.8487235e+20  9.2711797e+19 -3.8092853e+21  4.3977330e+20]\n",
      " [-6.2947415e+19  9.9783660e+18 -4.0997977e+20  4.7331174e+19]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "M = nd.random.normal(shape=(4,4))\n",
    "print('A single matrix', M)\n",
    "for i in range(100):\n",
    "    M = nd.dot(M, nd.random.normal(shape=(4,4)))\n",
    "\n",
    "print('After multiplying 100 matrices', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 소실(vanishing gradient)\n",
    "\n",
    "반대의 문제인 그래디언트 소실(vanishing gradient)도 나쁜 경우입니다. 주요 원인 중의 하나는 각 층의 선형 연산과 함께 엮이는 활성화 함수(activation function)  $\\sigma$ 입니다. 역사적으로는 [Multilayer Perceptrons](../chapter_deep-learning-basics/mlp.ipynb) 절에서 소개했던 sigmoid 함수  $(1 + \\exp(-x))$ 가 유명한 활성화 함수(activation function)였습니다. 이 함수를 비선형 활성화 함수(activation function)로 사용했을 때 문제가 될 수 있는지를 보기 위해서, 이 함수에 대해서 간단하게 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = nd.arange(-8.0, 8.0, 0.1)\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = x.sigmoid()\n",
    "y.backward()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x.asnumpy(), y.asnumpy())\n",
    "plt.plot(x.asnumpy(), x.grad.asnumpy())\n",
    "plt.legend(['sigmoid', 'gradient'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 보이는 것처럼 sigmoid의 그래디언트는 아주 큰 수나 아주 작은 수에서 소멸합니다. 체인룰(chain rule)로 인해서, 활성화(activation)들이 $[-4, 4]$ 범위에 들어가지 않지 않으면 전체 곱의 그래디언트(gradient)는 소멸될 수 있다는 것을 의미합니다. 층을 많이 사용하는 경우, 이 현상이 어떤 층에서 일어날 가능성이 높습니다.  ReLU $\\max(0,x)$ 가 소개되기 전까지는 이 문제가 딥 네트워크 학습의 단점이었습니다. 그 결과 ReLU 가 활성화(activation)로 설계할 때 기본 선택이 되었습니다.\n",
    "\n",
    "### 대칭성\n",
    "\n",
    "딥 네트워크 디자인의 마지막 문제는 파라미터화에 내재된 대칭입니다. 두 개의 은닉\n",
    "유닛(hidden unit), $h_1$ and $h_2$ 을 갖는 한 개의 은닉층(hidden layer)를 가지고\n",
    "있는 딥 네트워크를 가정하겠습니다. 이 경우, 첫번째 층의 가중치 $\\mathbf{W}_1$ 를\n",
    "뒤집고, 두번째 층의 결과도 뒤집을 경우, 동일한 함수를 얻게 됩니다. 좀 더\n",
    "일반적으로 각 층의 은닉 유닛(hidden unit) 간에는 치환 대칭성(permutation\n",
    "symmetry)이 존재합니다. 이것은 이론적으로만 말썽이되는 것이 아닙니다. 어떤 층의\n",
    "파라미터를 모두 0으로 초기화를 하거나 ($\\mathbf{W}_l = 0$ ),  $\\mathbf{W}_l$ 의\n",
    "모든 값을 동일하게 설정한다고 가정합니다. 이 경우, 모든 차원의\n",
    "그래디언트(gradient)들이 같게 되고, 주어진 층에 내재된 표현력을 전혀 사용할 수\n",
    "없게 됩니다. 사실, 그 은닉층(hidden layer)는 단일 유닛(single unit) 처럼\n",
    "동작합니다.\n",
    "\n",
    "## 파라미터 초기화\n",
    "\n",
    "위 문제를 해결하거나 최소한 완화시키는 방법은 가중치 벡터의 초기화를 조심하게 하는 것입니다. 이렇게 해서 적어도 초기의 그래디언트(gradient)가 소멸되지 않게 하고, 네트워크 가중치들이 너무 커지지 않게 합리적인 범위에 존재하게 할 수 있습니다. 최적화에서의 추가적 조치나 적합한 정규화(regularization)을 통해서 너무 나빠지는 것을 막을 수 있습니다. 이제 방법들에 대해서 알아보겠습니다.\n",
    "\n",
    "### 기본 초기화\n",
    "\n",
    "[“Concise Implementation of Linear Regression”](linear-regression-gluon.ipynb) 절에서 우리는 `net.initialize(init.Normal(sigma=0.01))` 을 이용해서 가중치의 초기값으로 정규 분포에서 임의의 수 선택하는 방법을 사용했습니다. 초기화 방법을 명시하지 않은 경우, 즉, `net.initialize()` 를 호출하는 경우에 MXNet은 기본 랜덤 초기화 방법을 적용합니다. 이는, 가중치의 각 원소는  $U[-0.07, 0.07]$ 범위의 균일 분포에서 선택된 값을 갖고, 편향(bias) 파라미터는 모두 0으로 설정됩니다. 일반적인 문제의 크기에서 이 두 방법은 상당히 잘 작동합니다.\n",
    "\n",
    "### Xavier 초기화\n",
    "\n",
    "어떤 층의 은닉 유닛(hidden unit) $h_{i}$에 적용된 활성화(activation)의 범위 분포를 살펴보겠습니다. 이 값들은 다음과 같이 계산됩니다.\n",
    "\n",
    "$$h_{i} = \\sum_{j=1}^{n_\\mathrm{in}} W_{ij} x_j$$\n",
    "\n",
    "가중치 $W_{ij}$ 들은 같은 분포에서 서로 독립적으로 선택됩니다. 이 분포는 평균이 0이고 분산이  $\\sigma^2$ 라고 가정하겠습니다. (하지만, 이 분포가 가우시안(Gaussian) 이어야 한다는 것은 아니고, 단지 평균과 분산이 필요할 뿐입니다.)  층의 입력  $x_j$ 를 제어할 수 있는 방법이 없지만, 그 값들의 평균이 0이고 분산이 $\\gamma^2$ 이고,  $\\mathbf{W}$ 과는 독립적이라는 다소 비현실적인 가정을 하겠습니다. 이 경우,  $h_i$ 의 평균과 분산을 다음과 같이 계산할 수 있습니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{E}[h_i] & = \\sum_{j=1}^{n_\\mathrm{in}} \\mathbf{E}[W_{ij} x_j] = 0 \\\\\n",
    "    \\mathbf{E}[h_i^2] & = \\sum_{j=1}^{n_\\mathrm{in}} \\mathbf{E}[W^2_{ij} x^2_j] \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} \\mathbf{E}[W^2_{ij}] \\mathbf{E}[x^2_j] \\\\\n",
    "        & = n_\\mathrm{in} \\sigma^2 \\gamma^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$n_\\mathrm{in} \\sigma^2 = 1$ 을 적용하면 분산을 고정시킬 수 있습니다. 이제 back propagation을 고려해봅니다. 가장 상위층들로부터 전달되는 그래디언트(gradient) 와 함께 비슷한 문제를 만나게 됩니다. 즉,  $\\mathbf{W} \\mathbf{w}$ 대신에  $\\mathbf{W}^\\top \\mathbf{g}$ 를 다뤄야합니다. 여기서 $\\mathbf{g}$ 는 상위층으로부터 전달되는 그래디언트(gradient)를 의미합니다. 포워드 프로퍼게이션(forward propagation)에서와 같은 논리로,  $n_\\mathrm{out} \\sigma^2 = 1$ 이 아닐 경우에는 그래디언트(gradient)의 분산이 너무 커질 수 있습니다. 이 상황이 우리를 다음과 같은 딜레마에 빠지게 합니다. 즉, 우리는 두 조건을 동시에 만족시킬 수 없습니다. 대신, 다음은 조건은 쉽게 만족시킬 수 있습니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ 또는 동일하게 }\n",
    "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "이것이 2010년에  [Xavier Glorot and Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 이 제안한 Xavier 초기화의 기본이 되는 논리입니다. 이 방법은 실제로 충분이 잘 작동합니다. 가우시안(Gaussian) 확률 변수에서 Xavier 초기화는 평균이 0이고 분산이  $\\sigma^2 = 2/(n_\\mathrm{in} + n_\\mathrm{out})$ 인 정규 분포에서 값을 선택합니다.  $U[-a, a]$ 에 균등하게 분포한 확률 변수의 경우, 분산이  $a^2/3$ 이 됩니다.  $a^2/3$ 을  $\\sigma^2$ 에 대한 조건에 대입하면 다음과 같은 분포의 초기화를 할 수 있습니다.\n",
    "\n",
    "$U\\left[-\\sqrt{6/(n_\\mathrm{in} + n_\\mathrm{out})}, \\sqrt{6/(n_\\mathrm{in} + n_\\mathrm{out})}\\right]$.\n",
    "\n",
    "### 그 외의 것들\n",
    "\n",
    "위 내용은 아주 일부입니다. MXNet의 `mxnet.initializer` 모듈은 10가지 이상의 경험적 방법들을 제공합니다. 이 방법들은 슈퍼 해상도(superresolution), 시퀀스(sequence) 모델 또는 관련된 문제들에서 파라미터들이 연관된 경우에 사용할 수 있습니다. 이 모듈이 제공하는 것들을 살펴보는 것을 권장합니다.\n",
    "\n",
    "## 요약\n",
    "\n",
    "* 그래디언트 소멸(Vanishing gradient)와 그래디언트 폭발(exploding gradient)은 아주 깊은 네트워크에서 발생하는 흔한 문제입니다. 이를 위해서 그래디언트(gradient)와 파라미터가 잘 통제되도록 하는 것이 중요합니다.\n",
    "* 초기화 방법은 최소한 초기의 그래디언트(gradient)들이 너무 커지거나 너무 작아지지 않도록 하는데 필요합니다.\n",
    "* ReLU는 그래디언트 소멸(vanishing gradient) 문제 중에 하나를 해결합니다. 즉, 매우 큰 입력에 대해서 그래디언트(gradient) 가 사라지는 것을 해결합니다. 이는 수렴을 아주 빠르게 가속화해줍니다.\n",
    "* 랜덤 초기화는 최적화를 수행하기 전 대칭을 깨 주는데 중요합니다.\n",
    "\n",
    "## 문제\n",
    "\n",
    "1. 치환 대칭성(permutation symmetry) 이외에 대칭성(symmetry)를 깨는 다른 사례를 디자인할 수 있나요?\n",
    "1. 선형 회귀나 softmax 회귀에서 모든 가중치 파라미터를 같은 값으로 초기화할 수 있나요?\n",
    "1. 두 행렬의 곱에서 고유값(eigenvalue)들의 해석적 범위(analytic bound)를 찾아보세요. 그래디언트(gradient)들을 잘 제어되도록 하는 것에 대해서 어떤 것을 알 수 있나요?\n",
    "1. 어떤 항들의 값이 커지고 있는 것을 알게 된 경우, 이 후에 이를 고칠 수 있나요?  [You, Gitman and Ginsburg, 2017](https://arxiv.org/pdf/1708.03888.pdf) 의 LARS 논문을 참고해보세요.\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2345)\n",
    "\n",
    "![](../img/qr_numerical-stability-and-init.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}